"""
Este m√≥dulo contiene herramientas esenciales para el procesamiento, an√°lisis y entrenamiento 
de modelos en datasets de im√°genes m√©dicas, con un enfoque particular en colonoscopias 
y detecci√≥n de p√≥lipos. Incluye clases y funciones para explorar datasets, preparar datos, 
procesar anotaciones, entrenar modelos y visualizar resultados.

### Contenido principal:

- `ImageDatasetProcessor`: gestiona datasets personalizados, cargando im√°genes y m√°scaras, 
  extrayendo estad√≠sticas, generando divisiones (train/val/test) y creando dataloaders.

- `TrainModel`: clase principal para entrenar modelos de detecci√≥n de objetos con PyTorch, 
  monitorizando m√©tricas como IoU y p√©rdida, y visualizando los resultados del entrenamiento.
  Usa dataloaders con el formato ImageDatasetProcessor

- Funciones auxiliares (`bbox_corn2cent`, `bbox_cent2corn`, etc.): conversi√≥n entre formatos 
  de bounding boxes y c√°lculo del IoU.

Este archivo es una base s√≥lida para construir un pipeline completo de procesamiento 
y entrenamiento con im√°genes m√©dicas segmentadas.
"""



import os
import cv2
import json
import csv
import json
import random
import torch
import pynvml
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.patches as patches 
import seaborn as sns
from datasets import Dataset
from PIL import Image
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from torchvision.ops import box_iou
import torchvision.transforms as transforms

class ImageDatasetProcessor:
    """
    Esta clase procesa el dataset dado guardandolo como un diccionario de elementos.
    Ofrece herramientas y funciones para a partir de este diccionario salvar, modificar
    o obtener informaci√≥n relevante de el dataset.
    Esta clase guarda el dataset procesado en un json en el path dado
    """

    def __init__(self, target_resolution=(256, 256),
                 dataset_name=None, json_path=None):
        """
        Esta funcion inicializa la clasee ImageDatasetProcessor que se debe usar 
        de estas dos maneras:
        1- Nuevo dataset, creamos la clase con json_path= a la direcci√≥n y nombre
        del json donde guardaremos los datos procesados. Para luego usar load_dataset
        para cargar todas los conjuntos de datos deseados.
        2- Dataset ya procesado, al crear la clase y aportar el json esta funci√≥n
        ya carga la informaci√≥n directamente de ah√≠.

        - target_resolution: reoluci√≥n del reescalado final de la imagen para 
            introducirla como tensor en el modelo, se usa para mostrar datos
        - format_doc: documento de texto con el tipo de luz de cada imagen del ds
        - dataset_name: nombre del dataset utilizado para procesar el dataset 
            de forma personalizada, admitidos: "Piccolo"
        - json_path: path al json del dataset, si no se aprota entonces se crea
        - polyp_paths: path a las imagenes de polipos, uno o varios directorios
        - mask_paths: path a las imagenes de macaras, uno o varios directorios
        - void_paths: path a las imagenes de void , uno o varios directorios
        """

        # contexto que nos da el usuario
        self.target_resolution = target_resolution      # ancho x alto
        self.json = json_path

        # Informaci√≥n sobre el dataset
        self.resolution_counts = {}     # Conteo de im√°genes por resoluci√≥n
        self.light_counts = {}          # N√∫mero de im√°genes por formato: WLI, NBI ...
        self.split_counts = {}          # N√∫mero de im√°genes por funcion; test, train ...
        self.channel_counts = {}        # Conteo de im√°genes por n√∫mero de canales
        self.brightness = []            # Brillo de las imagenes
        self.contrast = []              # Contraste de las imagenes
        self.polyp_centers = []         # Centros de los p√≥lipos
        # suma de la dist euclidea del centro del ·πïolipo respecto al centro img
        self.sum_mask_eucl_dist2center = 0
        # heatmap con la distribuci√≥n de las m√°scaras
        self.mask_heatmap = np.zeros(target_resolution, dtype=np.float32)
        # suma de porcentage de ocupaci√≥n de las m√°scaras en la imagen
        self.sum_masks_percentage = 0


        # diccionario de im√°genes y su formato
        self.dict = {}
        self.format = {
            "path": "",
            "mask_path": "",
            "void_path": "None",
            "size": (0, 0),
            "light_type": "Unknown",    # WL, NBI, BLI, ...
            "bbox": (0, 0, 0, 0),       # formato center (cx, cy, w, h)
            "split": "None",            # train, test, validation, no split
        }

        # json ya existe, por lo que sencillamente lo cargamos
        if os.path.isfile(self.json):
            self._load_from_json()


    def load_dataset(self, polyps_path, masks_path, voids_path=None, 
                     split="None", dir_light_type=None, light_csv=None):
        """
        Dado el directorio de las imagenes del dataset crea un diccionario con
        las imagenes de los polipos con informacion relevante como sus m√°scaras 
        , void si los hubiera, tama√±o, tipo de luz etc. y actualiza con esta
        informaci√≥n el diccionario del dataset dado, adem√°s guarda los datos
        procesados en un json si este ha sido aportado

        - polyps_path: directorio con im√°genes de p√≥lipos
        - masks_path: directorio con las m√°scaras de los p√≥lipos
        - voids_path voids de las im√°genes si los hubiera
        - split: a que split pertenece el directorio de imagenes
        - light_type: el tipo de luz de las im√°genes del directorio
        - light_type_file: ruta al csv con la clasificacion de luz de las imagenes
        nombre_img,tipo_de_luz
        """
        image_dict = {}

        # Primero guardamos una lista ordenada con todos los polypos y mascaras
        polyp_list = sorted(os.listdir(polyps_path))
        mask_list = sorted(os.listdir(masks_path))
        void_list = sorted(os.listdir(voids_path)) if voids_path else None

        # cargamos el csv si lo hubiera y la informaci√≥n
        light_types = {}
        if light_csv is not None:
            with open(light_csv, newline='', encoding='utf-8') as file:
                reader = csv.reader(file, delimiter=';')
                # formato "nombre imagen": tipo de luz
                for row in reader:
                    light_types[row[0]] = row[1].strip()

        # guardamos los datos del directorio dado con el formato del diccionario
        for i, polyp in enumerate(polyp_list):
            img_name = os.path.basename(polyp)
            img_path = os.path.join(polyps_path, polyp)
            mask_path = os.path.join(masks_path, mask_list[i])
            void_path = os.path.join(voids_path, void_list[i]) if void_list else None
            light_type = dir_light_type or light_types.get(img_name, "Unknown")

            # primero obtenemos el formato de la imagen
            img = Image.open(img_path)
            img_size = img.size

            # Ahora obtenemos la bbox
            mask = Image.open(mask_path)
            bbox = self._bbox_from_mask(mask)

            # actualizamos las estad√≠sticas del dataset
            self._update_stats(img, mask, void_path, light_type, split, img_name)
            img.close()     # cerramos las im√°genes
            mask.close()
            
            # Ahora guardamos todos los datos en el diccionario
            image_dict[img_name] = self.format.copy()
            image_dict[img_name]["path"] = img_path
            image_dict[img_name]["mask_path"] = mask_path
            image_dict[img_name]["void_path"] = void_path
            image_dict[img_name]["size"] = img_size
            image_dict[img_name]["light_type"] = light_type
            image_dict[img_name]["bbox"] = bbox
            image_dict[img_name]["split"] = split
            # print(f"Imagen procesada {img_name}: {i}")
        
        # finalmente guardamos los nuevos datos en el diccionario
        self.dict.update(image_dict)

        # y sobreescribimos el json con la nueva informaci√≥n si existe
        if self.json is not None:
            self._save_on_json()

    
    def get_dataloaders(self, batch_size, use_premade_splits=False, rand=False,
                        train_split=0, val_split=0, test_split=0):
        """
        Funci√≥n que divide el diccionario del dataset los tres conjuntos de train
        validation y test.
        
        - use_premade_splits: puedes usar el conjunto prehecho del datset si lo hay
        o indicar el porcentaje de cada split.
        - rand: si quieres mezclar aleatoriamente las im√°genes del dataset antes del split
        - train_split: num elementos del dataset para el conjunto de entrenamiento
        - val_split: num elementos del dataset para el conjunto de validacion
        - test_split: num elementos del dataset para el conjunto de test
        """

        train_ids = []
        val_ids = []
        test_ids = []
        
        # Comprovaciones de que se llama a la funci√≥n correctamente
        first_img = list(self.dict)[0]
        if use_premade_splits and self.dict[first_img]["split"] == "None":
            print("No hay un split prehecho para crear los dataloaders!")
            return None, None, None
        
        if train_split + val_split + test_split > len(self.dict):
            print(f"Splits indiccados superan el n√∫mero de elementos del dataset: {len(self.dict)}")

        # Primero elegimos las im√°genes de los splits
        image_ids = list(self.dict.keys())  # obtenemos los nombres de las im√°genes

        # Usamos el split de base
        if use_premade_splits:
            for img, data in self.dict.items():  # recorremos los valores
                if data["split"] == "train":
                    train_ids.append(img)
                elif data["split"] == "validation":
                    val_ids.append(img)
                else:
                    test_ids.append(img)
        # hacemos el split a mano, aleatorio o no
        else:
            if rand:
                # Obtenemos la cantidad de imagenes con las que queremos trabajar aleatoriamente
                selected_ids = np.random.choice(image_ids, 
                                                size=train_split+val_split+test_split, 
                                                replace=False)
                suffle=True
            else:   # cogemos las primeras del dataset
                selected_ids = image_ids[:test_split+val_split+train_split]
                suffle=False
            
            # Ahora, separamos los conjuntos
            train_val_ids, test_ids = train_test_split(selected_ids, 
                                                    test_size=test_split, 
                                                    suffle=suffle)
            # Separamos validation de train y terminamos con los 3 conjuntos
            train_ids, val_ids = train_test_split(train_val_ids, test_size=val_split, 
                                                suffle=suffle)

        # Obtenemos los dataset conteniendo cada uno las imagenes seleccionadas
        dtrain = self._dataset_from_dict_ids(train_ids)
        dval = self._dataset_from_dict_ids(val_ids)
        dtest = self._dataset_from_dict_ids(test_ids)

        # Finalmente creamos los dataLoaders de las im√°genes de cada split
        # suffle para mezclar los datos en cada √©poca 
        # pin_memory para acelerar CPU->GPU
        train_loader = DataLoader(dtrain, batch_size=batch_size, shuffle=True, pin_memory=True)
        val_loader = DataLoader(dval, batch_size=batch_size, shuffle=True, pin_memory=True)
        test_loader = DataLoader(dtest, batch_size=batch_size, shuffle=True, pin_memory=True)

        return train_loader, val_loader, test_loader
    

    def print_summary(self):
        """
        Imprime un resumen de las estad√≠sticas del dataset.
        """
        print(f"Total im√°genes: {len(self.dict)}")

        print("Composici√≥n del dataset:")
        for dictionary in [
                self.resolution_counts,
                self.light_counts, 
                self.split_counts, 
                self.channel_counts]:
            if dictionary == self.resolution_counts:
                print(f"Resoluci√≥nes: total distintas resoluciones {len(self.resolution_counts)}")
            elif dictionary == self.light_counts:
                print("Tipos de luz:")
            elif dictionary == self.split_counts:
                print("Splits:")
            else:
                print("Canales:")
            for data, num in dictionary.items():
                print(f"\t{data}: {num}", end="")
            print("\n")

        mean_masks_percentage = self.sum_masks_percentage / len(self.dict)
        print(f"Volumen medio de los p√≥lipos respecto a la imagen:\t{mean_masks_percentage}%")
        mean_mask_eucl_dist2center = self.sum_mask_eucl_dist2center / len(self.dict)
        print(f"Distancia media del centro del p√≥lipos al centro de la imagen:\t{mean_mask_eucl_dist2center}px")
    
    def graph_summmary(self):
        # Configuraci√≥n del estilo de los gr√°ficos
        sns.set(style="whitegrid")

        # Crear ventana con gr√°ficos
        fig, axs = plt.subplots(4, 2, figsize=(10, 8))


        # Graficamos los diagramas
        charts = [
            # Gr√°fico 1: Distribuci√≥n de las im√°genes por split
            (self.split_counts, axs[0, 0], 'Divisi√≥n de im√°genes del dataset', 'N√∫mero de Im√°genes'),
            # Gr√°fico 2: Composici√≥n del dataset por tipo de luz
            (self.light_counts, axs[0, 1], 'Composici√≥n del dataset por tipo de luz', 'N√∫mero de Im√°genes'),
            # Gr√°fico 3: Tipos de resoluciones en las im√°genes del dataset
            (self.resolution_counts, axs[1, 0], 'Tipos de resoluciones en las im√°genes del dataset', 'N√∫mero de Im√°genes'),
            # Gr√°fico 4: N√∫mero de canales por tipo de im√°gen
            (self.channel_counts, axs[1, 1], 'Formato de las im√°genes', 'N√∫mero de Im√°genes')
        ]

        for data, ax, title, ylabel in charts:
            ax.set_ylabel(ylabel)
            ax.bar(data.keys(), data.values(), color=['blue', 'green', 'orange'])
            ax.set_title(title)

        # Graficamos los histogramas
        hist = [
            # Gr√°fico 5: Histograma del brillo en las im√°genes
            (self.brightness, axs[2, 0], 'Brillo de los frames', 'N√∫mero de Im√°genes'),
            # Gr√°fico 6: Histograma del contraste en las im√°genes
            (self.contrast, axs[2, 1], 'Contraste de los frames', 'N√∫mero de Im√°genes')
        ]
        
        for data, ax, title, ylabel in hist:
            ax.hist(data, bins=20, color='forestgreen')
            ax.set_title(title)
            ax.set_ylabel(ylabel)


        # Gr√°fico 7: heatmap de distribuci√≥n de las m√°scaras
        sns.heatmap(self.mask_heatmap, cmap="crest", ax=axs[3, 0], cbar=True,
                    xticklabels=False, yticklabels=False)
        axs[3, 0].set_title('Distribuci√≥n de las m√°scaras (Heatmap)')
        # Gr√°fico 8: muestra de los centros de los p√≥lipos
        cx, cy = zip(*self.polyp_centers)       # dos listas con coordenadas
        axs[3, 1].scatter(cx, cy, c='lightblue', alpha=0.5, s=10)
        axs[3, 1].set_title('Distribuci√≥n de los centros de las m√°scaras (Heatmap)')
        axs[3, 1].set_xlim([0, self.target_resolution[0]])  # Ajustamos los ejes para ser como la im√°gen
        axs[3, 1].set_ylim([0, self.target_resolution[1]]) 

        # Ajustar el layout
        plt.tight_layout()
        plt.show()

    
    def show_image(self, id):
        """
        Esta funcion muestra la imagen con la bbox asociada
        Mostramos la imagen id si tenemos una prediccion de YOLO la muestra tambi√©n
        """
        img_path = self.dict[id]['path']
        img_w, img_h = self.dict[id]['size']
        x, y, w, h = bbox_cent2corn(self.dict[id]['bbox'], img_w, img_h)
        img = mpimg.imread(img_path)

        print(f"Imagen {id}\tbbox: {(x, y, w, h)}")

        # Creamos la figura para a√±adir los datos
        fig, ax = plt.subplots(1) 
  
        # Cargamos la imagen en la figura
        ax.imshow(img)

        # A√±adimos el dibujo de la bbox
        rect = patches.Rectangle((x, y), w, h, linewidth=2, 
                                edgecolor='cyan', facecolor="none") 
        
        # Add the patch to the Axes 
        ax.add_patch(rect) 
        plt.show()
    

###########################    FUNCIONES PRIVADAS    ###########################


    def _dataset_from_dict_ids(self, ids):
        """
        Esta funcion devuelve un dataset con los ids de imagenes dados.
        Para ello usa la funcion Dataset.from_dict
        """
        # Convertimos el diccionario en un formato adecuado para `from_dict()`
        # Cada clave ser√° una lista de valores para cada campo
        data = {
            'path': [],     # input, imagen del dataset
            'bbox': []      # output la bbox en formato center (cx,cy,w,h)
        }

        # Llenamos el diccionario con los datos de `final_dict`
        for image_id, image_data in self.dict.items():
            if image_id in ids:
                data['path'].append(image_data['path'])
                data['bbox'].append(image_data['bbox'])

        # Ahora creamos el Dataset
        return Dataset.from_dict(data)


    def _update_stats(self, img, mask, void_path, light_type, split, img_name):
        """
        Actualiza las estad√≠sticas con informacion util del dataset: 
            - Resoluci√≥n: tipos de resoluciones y cantidad de imagenes en ese formato
            - N√∫mero de canales: distribucion de imagenes por numero de canales
            - N√∫mero de imagen por tipo de luz
            - Distribucion de las m√°scaras:
            Donde se encuentran las m√°scaras distribuidas en las im√°genes
            y el porcentaje de la im√°gen que cubre el p√≥lipo
        """
        # 1Ô∏è‚É£- Resoluci√≥n de la im√°gen
        resolution = f"{img.size[1]}x{img.size[0]}"
        self._update_stat_dict(self.resolution_counts, resolution)

        # 2Ô∏è‚É£- Tipo de luz de la im√°gen
        self._update_stat_dict(self.light_counts, light_type)

        # 3Ô∏è‚É£- Split al que pertenece la imagen
        self._update_stat_dict(self.split_counts, split)

        # 4Ô∏è‚É£- N√∫mero de canales de PIL por im√°gen de las tres im√°genes
        chan_type_img = f"polyp_{img.mode}"         # polyp
        self._update_stat_dict(self.channel_counts, chan_type_img)
        chan_type_mask = f"mask_{mask.mode}"         # mask
        self._update_stat_dict(self.channel_counts, chan_type_mask)
        if void_path is not None:                   # void si lo hay
            with Image.open(void_path) as void_img:
                chan_type_void = f"void_{void_img.mode}"         # mask
                self._update_stat_dict(self.channel_counts, chan_type_void)

        # 5Ô∏è‚É£- Obtenemos el brillo de la imagen
        # es lo mismo que la media de la escala de grises
        gray_img = img.convert("L")    # L es escala de grises de PIL
        gray_pixels = np.array(gray_img)
        self.brightness.append(np.mean(gray_pixels))

        # 6Ô∏è‚É£- Contraste de la im√°gen
        # El contraste es la desviaci√≥n est√°ndar de los p√≠xeles
        self.contrast.append(np.std(gray_pixels))

        # 7Ô∏è‚É£- Porcentaje de ocupaci√≥n del p√≥lipo en pantalla
        gray_img = mask.convert("L")    # L es escala de grises de PIL
        gray_pixels = np.array(gray_img)
        mask_pixels = np.sum(gray_pixels==255)      # cu√°ntos pixeles son p√≥lipo
        percentage = (mask_pixels / gray_pixels.size) * 100
        self.sum_masks_percentage += percentage

        # 8Ô∏è‚É£- Actualizar el heatmap de disposici√≥n de los p√≥lipos respecto 
        # a la im√°gen
        mask_resized = mask.resize(self.target_resolution, resample=Image.NEAREST)
        mask_norm = np.array(mask_resized) / 255.0
        self.mask_heatmap += mask_norm.transpose()  # np cambia a alto x anchoS

        # 9Ô∏è‚É£- Actualizar la desviaci√≥n del centro del ·πïolipo
        mask_cx, mask_cy = self._get_mask_center(np.array(mask_resized), img_name)
        # guardamos la desviaci√≥n respecto del centro
        img_cx = self.target_resolution[0]/2
        img_cy = self.target_resolution[1]/2

        # distancia eucl√≠dea al centro
        dist = np.sqrt((img_cx - mask_cx)**2 + (img_cy - mask_cy)**2)
        self.sum_mask_eucl_dist2center += dist

        # üîü- Guardamos el centro del p√≥lipo para represntarlo
        self.polyp_centers.append((mask_cx, mask_cy))


    def _update_stat_dict(self, dict, data):
        """
        Actualiza el diccionario de estad√≠sticas dado, con el dato "data"
        """
        if data not in dict:
            dict[data] = 0      # creamos una nueva entrada en el diccionario
        dict[data] += 1


    def _get_mask_center(self, mask, img_name):
        """
        Obtiene el centro ponderado de la m√°scara dada, esto es la media de
        los valores de la m√°scara, devuelve los dos centros
        """
        # obtenemos las coordenadas de los p√≠xeles de la m√°scara
        y, x = np.where(mask > 0)

        if len(x) == 0 and len(y) == 0:
            cx = self.target_resolution[0]
            cy = self.target_resolution[1]
            # print(f"image {img_name} m√°scara vac√≠a")
        else:
            # devolvemos el centro de la m√°scara
            cx = round(np.mean(x)) # centro redondeado
            cy = round(np.mean(y)) # centro redondeado

        return cx, cy

    
    def _bbox_from_mask(self, mask):
        """
        Dada una m√°scara devuelve las coordenadas de la bbox con el formato de
        YOLO: (cx, cy, anchura, altura) todo ello normalizado.
        """
        mask_w, mask_h = mask.size
        mask_array = np.array(mask)  # Convertir la m√°scara en array NumPy
        rows, cols = np.where(mask_array > 0)  # Encontrar p√≠xeles no negros

        if len(rows) == 0 or len(cols) == 0:
            return (0, 0, 0, 0)  # Si la m√°scara est√° vac√≠a

        # Coordenadas m√≠nimas y m√°ximas
        min_x, max_x = cols.min(), cols.max()
        min_y, max_y = rows.min(), rows.max()

        # Ancho y alto
        width = max_x - min_x
        height = max_y - min_y

        # obtenemos el formato center a partir del corner
        corner_bbox = [min_x, min_y, width, height]
        bbox = bbox_corn2cent(corner_bbox, mask_w, mask_h)

        return bbox
    

    def _load_from_json(self):
        """
        Cargamos los datos del dataset desde el json
        """
        # cargamos el json
        with open(self.json, "r", encoding="utf-8") as json_file:
            json_dict = json.load(json_file)
        
        # vamos guardando cada dato
        self.resolution_counts = json_dict["resolution_counts"]
        self.light_counts = json_dict["light_counts"]
        self.split_counts = json_dict["split_counts"]
        self.channel_counts = json_dict["channel_counts"]
        self.brightness = json_dict["brightness"]
        self.contrast = json_dict["contrast"]
        self.polyp_centers = json_dict["polyp_centers"]
        self.sum_mask_eucl_dist2center = json_dict["sum_mask_eucl_dist2center"]
        self.mask_heatmap = np.array(json_dict["mask_heatmap"])
        self.sum_masks_percentage = json_dict["sum_masks_percentage"]
        self.dict = json_dict["dict"]

    
    def _save_on_json(self):
        """
        Guardamos los datos en el json con formato:
        {
            datos de analisis del dataset
            diccionario de im√°genes{}
        }
        """
        json_dict = {
            "resolution_counts": self.resolution_counts,
            "light_counts": self.light_counts,
            "split_counts": self.split_counts,
            "channel_counts": self.channel_counts,
            "brightness": self.brightness,
            "contrast": self.contrast,
            "polyp_centers": self.polyp_centers,
            "sum_mask_eucl_dist2center": self.sum_mask_eucl_dist2center,
            "mask_heatmap": self.mask_heatmap.tolist(),     # ya que es un np array
            "sum_masks_percentage": self.sum_masks_percentage,
            "dict": self.dict
        }

        with open(self.json, "w", encoding="utf-8") as json_file:
            json.dump(json_dict, json_file)  # `indent=4` para formato legible


class TrainModel:
    """
    Esta clase se encarga del proceso de entrenamiento del modelo dado unos par√°metros
    de entrenamiento y unos dataloaders, devolviendo los datos del entrenamiento del modelo
    """

    def __init__(self, model, loss_fn, optim):
                 
        """
        Iniciamos la clase especificando los datos de entrenamiento del modelo.
        model: modelo en pytorch en modo entrenamiento
        """
        self.model = model
        self.loss_fn = loss_fn
        self.optim = optim
        # usamos la GPU mejor si hay libre
        if torch.cuda.is_available():
            gpu_id = self._get_free_gpu()
            self.device = torch.device(f"cuda:{gpu_id}")
            print(f"Entrenando en GPU {gpu_id}")
        else:
            self.device = torch.device("cpu")
            print("Entrenando en CPU")

        # cargamos el modelo en el dispositivo
        self.model.to(self.device)

    def train_model(self, num_epoch, train_resolution, 
                    train_dataloader, validation_dataloader, test_dataloader,
                    silent = False):
        """
        Entrenamos el modelo dado con los par√°metros especificados
        train_resolution: resolucion a la que transformar las imagenes.
        data_loaders: en formato de ImageDatasetProcessor
        silent: para entrenar el modelo sin prompts
        """
        train_dl = train_dataloader
        test_dl = test_dataloader
        val_dl = validation_dataloader


        # definimos la transofrmacion para el tensor, en 256x256 ya que son patches de 16x16
        transform = transforms.Compose([
            transforms.Resize(train_resolution),
            transforms.ToTensor(),
        ])

        # Estas son variables para analizar el modelo
        log_epochs = 1 # cada cuantas epocas obtenemos datos del modelo
        loss_hist_train = [0] * num_epoch
        loss_hist_val = [0] * num_epoch
        IoU_hist_train = [0] * num_epoch
        IoU_hist_val = [0] * num_epoch

        for epoch in range(num_epoch):  # N√∫mero de √©pocas
            # üìç Entrenamos el modelo
            model_results = self._try_model(train_dl, self.device, self.model, 
                                            transform, train_mode=True, 
                                            loss_fn=self.loss_fn, 
                                            optimizer=self.optim, 
                                            img_size=train_resolution)
            
            # Guardamos los resultados de la √©poca
            loss_hist_train[epoch], IoU_hist_train[epoch] = model_results

            # üíæ Validamos el modelo
            model_results = self._try_model(val_dl, self.device, self.model, 
                                            transform, loss_fn=self.loss_fn, 
                                            img_size=train_resolution)
            loss_hist_val[epoch], IoU_hist_val[epoch] = model_results

            # mostramos como va el entrenamiento
            if not silent and epoch % log_epochs==0:
                print(f'Epoch {epoch}  Loss train {loss_hist_train[epoch]:.4f}  IoU train {IoU_hist_train[epoch]:.4f} ')
                print(f'Epoch {epoch}  Loss valid {loss_hist_val[epoch]:.4f}  IoU valid {IoU_hist_val[epoch]:.4f} ')

        # üèÅ Finalmente evaluamos el modelo en test
        model_results = self._try_model(test_dl, self.device, self.model, 
                                        transform, loss_fn=self.loss_fn, 
                                        img_size=train_resolution)
        loss_test, IoU_test = model_results

        if not silent:
            self._show_test_results(loss_test, IoU_test)

        # devolvemos los datos para su an√°lisis
        results = { 
            "loss_test": loss_test, 
            "IoU_test": IoU_test,
            "loss_hist_train": loss_hist_train,
            "IoU_hist_train": IoU_hist_train,
            "loss_hist_val": loss_hist_val,
            "IoU_hist_val": IoU_hist_val }

        return results
    
    def show_results(self, dict, save_img=False, img_name="Tmp_res.png"):
        """
        Mostramos los resultados dados como una gr√°fica, y mostramos el resultado
        final por texto. dado el diccionario results con el formato de "train_model"
        """
        loss_test = dict["loss_test"] 
        IoU_test = dict["IoU_test"]
        loss_hist_train = dict["loss_hist_train"]
        loss_hist_val = dict["loss_hist_val"]
        IoU_hist_train = dict["IoU_hist_train"]
        IoU_hist_val = dict["IoU_hist_val"]

        num_epoch = len(loss_hist_train)

        # Graficar la evoluci√≥n de la Loss durante el entrenamiento y validaci√≥n
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)  # Subgr√°fico 1: Loss
        plt.plot(range(num_epoch), loss_hist_train, label='Loss train', color='blue')
        plt.plot(range(num_epoch), loss_hist_val, label='Loss valid', color='red')
        plt.title('Loss durante el entrenamiento y validaci√≥n')
        plt.xlabel('√âpocas')
        plt.ylabel('Loss')
        plt.legend()

        # Graficar la evoluci√≥n de la IoU durante el entrenamiento y validaci√≥n
        plt.subplot(1, 2, 2)  # Subgr√°fico 2: IoU
        plt.plot(range(num_epoch), IoU_hist_train, label='IoU train', color='blue')
        plt.plot(range(num_epoch), IoU_hist_val, label='IoU valid', color='red')
        plt.title('IoU durante el entrenamiento y validaci√≥n')
        plt.xlabel('√âpocas')
        plt.ylabel('IoU')
        plt.legend()

        plt.tight_layout()

        # guardamos la imagen si es necesario
        if save_img:
            plt.savefig(img_name, format='png', dpi=300)

        # Mostrar ambas gr√°ficas
        plt.show()


        self._show_test_results(loss_test, IoU_test)

    def _try_model(self, data_loader, device, model, transform, train_mode=False, 
              loss_fn=None, optimizer=None, img_size=(240, 240)):
        """
        Esta funcion se encarga de correr en el modelo el dataloader proporcionado
        aplicando a las imagenes la transformaci√≥n dada, ejecutando todo en el dispositivo
        indicado y entrenandolo si esta indicado. 
        Si NO se indica entrenar, funciona como una validacion
        """

        # Para seguir el accurracy y el loss del modelo
        loss_try = 0
        IoU_try = 0

        # Escalador para ampliar los gradientes y usar float16 sin perder datos (vanishing de pesos cercanos a 0)
        scaler = torch.cuda.amp.GradScaler()

        for batch in data_loader:
            # Primero debemos cargar las imagen desde su path y convertirlas a tensores
            images = []
            
            # para ello cargamos las imagenes del batch en una lista
            for path in batch['path']:
                image = Image.open(path).convert('RGB') # Aseguramos 3 canales
                # guardamos la imagen transformada
                images.append(transform(image))
            
            # las convertimos en un tensor
            images = torch.stack(images)

            # Ahora procesamos las bbox que parecido a las imagenes vienen como una lista de tensores
            # por lo que las agrupamos y convertimos a un solo tensor
            bbox = torch.stack(batch['bbox']).T

            # Guardamos en GPU
            images = images.to(device, non_blocking=True)   # acelerado el paso a GPU
            bbox = bbox.to(device, non_blocking=True)

            # mixed precision mode, usamos float16 pero reescalamos para evitar perder datos cercanos a 0
            with torch.cuda.amp.autocast():
                pred = model(images)['pred_bboxes']
                loss = loss_fn(bbox, pred)

            if train_mode:
                scaler.scale(loss).backward()       # backward con amplificaci√≥n
                scaler.step(optimizer)              # actualizamos pesos
                scaler.update()                     # actualizamos escala
                optimizer.zero_grad()

            # Finalmente guardamos el error del batch para analizarlo
            loss_try += loss.item()

            # Obtenemos el valor e IoU del batch
            for pred_box, target_box in zip(pred, bbox):
                IoU_try += yolo_bbox_iou(img_size, pred_box.tolist(), target_box.tolist())

            # üîª Limpiamos la VRAM
            del images, bbox, pred, loss
            torch.cuda.empty_cache()

        # Obtenemos la media de error en entrenamiento
        loss_try /= len(data_loader.dataset)
        IoU_try /= len(data_loader.dataset)

        return (loss_try, IoU_try)
    
    def _show_test_results(self, loss_test, IoU_test):
        print("End of training!")
        print("-------------------- FINAL RESULTS ------------------------")
        print(f"|     - Test loss:     {loss_test}                         |")
        print(f"|     - Test IoU: {IoU_test}                         |")
        print("-----------------------------------------------------------")
    
    def _get_free_gpu(self):
        """
        Esta funci√≥n obtiene la gpu con menor carga de trabajo para evitar errores
        en entrenamiento.
        """
        pynvml.nvmlInit()   # iniciamos ell an√°lisis
        num_devices = pynvml.nvmlDeviceGetCount()   # get the number of GPUs
    
        max_free_mem = 0
        best_gpu = 0

        # Buscamos cu√°l es la GPU con m√°s VRAM disponible
        for i in range(num_devices):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
            free_mem = meminfo.free
            if free_mem > max_free_mem:
                max_free_mem = free_mem
                best_gpu = i
        
        pynvml.nvmlShutdown()   # terminamos el an√°lisis
        return best_gpu

    


############################    Herramientas    ############################

def show_Nresults(list_dict_res, list_dict_names, save_img=False, img_name="Tmp_res.png"):
    """
    Mostramos los resultados de la lista de diccionarios dada, siendo cada diccionario
    el resultado de un benchmark, y mostramos el resultado en una gr√°fica.
    dado la lista de diccionarios con el formato de "train_model"
    """

    loss_test_mean = 0
    IoU_test_mean = 0
    num_dicts = len(list_dict_res)
    colors = ["blue", "orange", "green", "red", "purple", "brown", "pink", 
              "gray", "olive", "cyan"]

    plt.figure(figsize=(24, 12))

    # 1Ô∏è‚É£- Loss train
    plt.subplot(2, 2, 1)
    # mostramos cada una de las muestras
    for i, dict in enumerate(list_dict_res):
        loss_hist_train = dict["loss_hist_train"]
        plt.plot(range(len(loss_hist_train)), loss_hist_train, 
                 label=list_dict_names[i], color=colors[i])
    plt.title('Loss Train')
    plt.xlabel('√âpocas')
    plt.ylabel('Loss')
    plt.legend()

    # 2Ô∏è‚É£- Loss validation
    plt.subplot(2, 2, 3)
    # mostramos cada una de las muestras
    for i, dict in enumerate(list_dict_res):
        loss_hist_val = dict["loss_hist_val"]
        plt.plot(range(len(loss_hist_val)), loss_hist_val, 
                 label=list_dict_names[i], color=colors[i])
    plt.title('Loss Validaci√≥n')
    plt.xlabel('√âpocas')
    plt.ylabel('Loss')
    plt.legend()

    # 3Ô∏è‚É£- IoU train
    plt.subplot(2, 2, 2)
    # mostramos cada una de las muestras
    for i, dict in enumerate(list_dict_res):
        IoU_hist_train = dict["IoU_hist_train"]
        plt.plot(range(len(IoU_hist_train)), IoU_hist_train, 
                 label=list_dict_names[i], color=colors[i])
    plt.title('IoU Train')
    plt.xlabel('√âpocas')
    plt.ylabel('Loss')
    plt.legend()

    # 4Ô∏è‚É£- Loss validation
    plt.subplot(2, 2, 4)
    # mostramos cada una de las muestras
    for i, dict in enumerate(list_dict_res):
        IoU_hist_val = dict["IoU_hist_val"]
        plt.plot(range(len(IoU_hist_val)), IoU_hist_val, 
                 label=list_dict_names[i], color=colors[i])
    plt.title('IoU Validaci√≥n')
    plt.xlabel('√âpocas')
    plt.ylabel('Loss')
    plt.legend()       

    plt.tight_layout()

    # guardamos la imagen si es necesario
    if save_img:
        plt.savefig(img_name, format='png', dpi=300)

    # Mostrar ambas gr√°ficas
    plt.show()


def load_json_dict(json_path):
    """
    Cargamos los datos del dataset desde el json dado, devolvermos un diccionario
    con los datos del json
    """
    # cargamos el json
    with open(json_path, "r", encoding="utf-8") as json_file:
        json_dict = json.load(json_file)

    return json_dict
    


def bbox_corn2cent(bbox, img_w, img_h):
    """"
    Esta funcion procesa las bboxes del formato "corner" usado en COCO [x, y, w, h] (0-num pixels)
    a formato "center" usado en la salida de YOLO [cx, cy, w, h] normalizado (0-1)
    """
    x, y, w, h = bbox

    # Obtenemos el centro de la bbox
    cx = x + (w / 2)
    cy = y + (h / 2)

    # normalizamos los datos
    cx_norm = cx / img_w
    cy_norm = cy / img_h
    w_norm = w / img_w
    h_norm = h / img_h

    # Devolvemos el formato de YOLO
    return [cx_norm, cy_norm, w_norm, h_norm]


def bbox_cent2corn(cent_bbox, img_w, img_h):
    """
    Esta funcion pasa del formato "center" usado en YOLO noramalizado de [cx, cy, w, h]
    al formato "corner" usado en COCO en coordenadas de la imagen [x, y, w, h] -> xy esq sup izq
    """
    cx_cent, cy_cent, w_cent, h_cent= cent_bbox


    w = int(w_cent * img_w)
    h = int(h_cent * img_h)
    cx = cx_cent * img_w
    cy = cy_cent * img_h
    x = cx - (w / 2)
    y = cy - (h / 2)

    # Hacemos los n√∫meros enteros para evitar fallos
    return [int(x), int(y), int(w), int(h)]

def bbox_corn2doublecorn(corn_bbox):
    """
    Esta funcion pasa del formato "corner" usado en COCO [x, y, w, h] -> xy esq sup izq
    al formato "double corner" usado en IoU de PyTorch [minx, miny, maxx, maxy]
    """
    x_min, y_min, w, h= corn_bbox
    
    x_max = x_min + w
    y_max = y_min + h

    # Hacemos los n√∫meros enteros para evitar fallos
    return [int(x_min), int(y_min), int(x_max), int(y_max)]

def bbox_cent2doublecorn(cent_bbox, img_w, img_h):
    """
    Esta funcion pasa del formato "center" usado en YOLO noramalizado de [cx, cy, w, h]
    al formato "double corner" usado en IoU de PyTorch [minx, miny, maxx, maxy]
    """
    # Primero pasamos ambas bbox de formato yolo (cx, cy, w, h) normalizado
    # al formato de bbox de IoU (xmin, ymin, xmax, ymax)
    dcorn_bbox = bbox_corn2doublecorn(bbox_cent2corn(cent_bbox, img_w, img_h))

    # restringimos los valores de pred_dcorn por si son negativos
    dcorn_bbox = [max(0, dcorn_bbox[0]), max(0, dcorn_bbox[1]), 
                  max(0, dcorn_bbox[2]), max(0, dcorn_bbox[3])]

    return dcorn_bbox

def yolo_bbox_iou(img_size, targ_box, pred_box):
    """
    Obtiene el inidce IoU de coincidencia de las bbox dadas
    """
    # Primero transformamos las bboxes para IoU "double corner" (xmin,ymin,xmax,ymax)
    pred_dcorn = bbox_cent2doublecorn(pred_box, img_size[0], img_size[1])
    targ_dcorn = bbox_cent2doublecorn(targ_box, img_size[0], img_size[1])

    pred_t = torch.tensor(pred_dcorn).unsqueeze(0)  # guardamos todas las bboxes en un tensor
    targ_t = torch.tensor(targ_dcorn).unsqueeze(0)

    # calculamos el IoU de las bbox
    iou_res = torch.nan_to_num(box_iou(pred_t, targ_t), nan=0.0)# evitamos nan si es 0
    return torch.sum(iou_res).item()
